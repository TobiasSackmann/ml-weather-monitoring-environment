{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(1, '../library')\n",
    "import database_helper\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"user\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"password\"\n",
    "mlflow.set_tracking_uri(uri=\"http://mlflow.local:80\")\n",
    "mlflow.set_experiment(\"Multi-Output Time Series Forecast\")\n",
    "mlflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('selected_columns.pkl', 'rb') as f:\n",
    "    selected_columns = pickle.load(f)\n",
    "\n",
    "strings_to_exclude = ['icon', 'moon', 'warning']\n",
    "selected_columns = [item for item in selected_columns if not any(substring in item for substring in strings_to_exclude)]\n",
    "\n",
    "strings_to_include = ['days_0', '10838']\n",
    "selected_columns = [item for item in selected_columns if all(substring in item for substring in strings_to_include)]\n",
    "\n",
    "print(str(len(selected_columns)) + ' Features selected')\n",
    "df = database_helper.query_data(field_list=selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('_time', inplace=True)\n",
    "df = df.select_dtypes(include='float64')\n",
    "df.interpolate(inplace=True)\n",
    "df = df.resample('h').mean()\n",
    "df.reset_index(inplace=True)\n",
    "date_time = pd.to_datetime(df.pop('_time'), format='%d.%m.%Y %H:%M:%S')\n",
    "\n",
    "display(df)\n",
    "display(date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
    "plt.plot(np.array(df['Day sin'])[:])\n",
    "plt.plot(np.array(df['Day cos'])[:])\n",
    "plt.xlabel('Time [h]')\n",
    "plt.title('Time of day signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "\n",
    "n = len(df)\n",
    "train_df = df[0:int(n*0.7)]\n",
    "val_df = df[int(n*0.7):int(n*0.9)]\n",
    "test_df = df[int(n*0.9):]\n",
    "\n",
    "num_features = df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "val_df = (val_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "               label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_window(self, features):\n",
    "  inputs = features[:, self.input_slice, :]\n",
    "  labels = features[:, self.labels_slice, :]\n",
    "  if self.label_columns is not None:\n",
    "    labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)\n",
    "\n",
    "  # Slicing doesn't preserve static shape information, so set the shapes\n",
    "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "  inputs.set_shape([None, self.input_width, None])\n",
    "  labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "  return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(self, data):\n",
    "  data = np.array(data, dtype=np.float32)\n",
    "  ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "      data=data,\n",
    "      targets=None,\n",
    "      sequence_length=self.total_window_size,\n",
    "      sequence_stride=1,\n",
    "      shuffle=False,\n",
    "      batch_size=32,)\n",
    "  ds = ds.map(self.split_window)\n",
    "  return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def train(self):\n",
    "  return self.make_dataset(self.train_df)\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "  return self.make_dataset(self.val_df)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "  return self.make_dataset(self.test_df)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "  result = getattr(self, '_example', None)\n",
    "  if result is None:\n",
    "    # No example batch was found, so get one from the `.train` dataset\n",
    "    result = next(iter(self.train))\n",
    "    # And cache it for next time\n",
    "    self._example = result\n",
    "  return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "WindowGenerator.val = val\n",
    "WindowGenerator.test = test\n",
    "WindowGenerator.example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_range = 48\n",
    "shift = 1\n",
    "wide_window = WindowGenerator(input_width=time_range, label_width=time_range, shift=shift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 20\n",
    "\n",
    "def compile_and_fit(model, window, patience=5):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n",
    "  model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(), metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS, validation_data=window.val, callbacks=[early_stopping])\n",
    "  return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualWrapper(tf.keras.Model):\n",
    "  def __init__(self, model):\n",
    "    super().__init__()\n",
    "    self.model = model\n",
    "\n",
    "  def call(self, inputs, *args, **kwargs):\n",
    "    delta = self.model(inputs, *args, **kwargs)\n",
    "    # The prediction for each time step is the input\n",
    "    # from the previous time step plus the delta\n",
    "    # calculated by the model.\n",
    "    return inputs + delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wide_window.test_df.shape)\n",
    "print(wide_window.train_df.shape)\n",
    "print(wide_window.val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_lstm = ResidualWrapper(\n",
    "    tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    tf.keras.layers.Dense(\n",
    "        num_features,\n",
    "        # The predicted deltas should start small.\n",
    "        # Therefore, initialize the output layer with zeros.\n",
    "        kernel_initializer=tf.initializers.zeros()\n",
    "        )\n",
    "]))\n",
    "\n",
    "history, model = compile_and_fit(residual_lstm, wide_window)\n",
    "model.save('./waether-timeseries-forecast.keras')  # The file needs to end with the .keras extension\n",
    "\n",
    "\n",
    "val_performance = {}\n",
    "performance = {}\n",
    "val_performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.val, return_dict=True)\n",
    "performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.test, verbose=0, return_dict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_df = wide_window.test_df[:time_range]\n",
    "true_data_df = wide_window.test_df[time_range:2*time_range]\n",
    "true_data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "input_data = np.reshape(input_data_df, (1, time_range, 13))\n",
    "predictions = model.predict(input_data)\n",
    "predictions_df = pd.DataFrame(predictions[0])\n",
    "predictions_df.columns = true_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_actual_vs_predicted(df_actual, df_predicted):\n",
    "    columns = df_actual.columns  # Get the column names\n",
    "    num_columns = len(columns)\n",
    "\n",
    "    # Calculate the number of rows needed (2 columns per row)\n",
    "    ncols = 2\n",
    "    nrows = (num_columns + 1) // ncols  # Ensure enough rows\n",
    "\n",
    "    # Adjust the figure size (width, height per plot)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 4 * nrows))\n",
    "    \n",
    "    # Flatten axes to make iteration easier, in case there's more than one row\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Iterate through each column\n",
    "    for i, col in enumerate(columns):\n",
    "        sns.lineplot(ax=axes[i], x=df_actual.index, y=df_actual[col], label='Actual', color='blue')\n",
    "        sns.lineplot(ax=axes[i], x=df_predicted.index, y=df_predicted[col], label='Predicted', color='orange')\n",
    "        axes[i].set_title(f'Actual vs Predicted for {col}')\n",
    "        axes[i].legend()\n",
    "\n",
    "    # Remove any unused axes if the number of columns isn't divisible by 2\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_actual_vs_predicted(true_data_df, predictions_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
